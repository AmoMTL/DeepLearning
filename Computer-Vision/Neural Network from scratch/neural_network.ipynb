{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will implement a simple neural network with two layers - a hidden layer with 16 nodes and an output layer with 10 nodes (as we are classifying numbers from *0-9*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of x_train: (784, 60000)\n",
      "Dimensions of y_train: (1, 60000)\n",
      "Dimensions of x_test: (784, 10000)\n",
      "Dimensions of y_test: (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import struct\n",
    "from array import array\n",
    "\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)\n",
    "    \n",
    "input_path = 'data'\n",
    "training_images_filepath = r\"data\\train-images-idx3-ubyte\\train-images-idx3-ubyte\"\n",
    "training_labels_filepath = r\"data\\train-labels-idx1-ubyte\\train-labels-idx1-ubyte\"\n",
    "test_images_filepath = r\"data\\t10k-images-idx3-ubyte\\t10k-images-idx3-ubyte\"\n",
    "test_labels_filepath = r\"data\\t10k-labels-idx1-ubyte\\t10k-labels-idx1-ubyte\"\n",
    "\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "# Convert to numpy arrays and flatten the 28x28 images\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1).transpose()\n",
    "y_train = y_train.reshape(y_train.shape[0], -1).transpose()\n",
    "x_test = x_test.reshape(x_test.shape[0], -1).transpose()\n",
    "y_test = y_test.reshape(y_test.shape[0], -1).transpose()\n",
    "print(\"Dimensions of x_train:\", x_train.shape)\n",
    "print(\"Dimensions of y_train:\", y_train.shape)\n",
    "print(\"Dimensions of x_test:\", x_test.shape)\n",
    "print(\"Dimensions of y_test:\", y_test.shape)\n",
    "\n",
    "_, m = x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 60000, $28\\times28$ pixel images in the training dataset which have been flattened to $784$ pixels. *x_train* has dimensions of $(784\\times60000)$.\n",
    "*y_train* has dimensions of $60000\\times1$, with the columns containing the class label for the images in *x_train*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations for the first  hidden layer,\n",
    "\n",
    "\\begin{align*}\n",
    "Z^{[1]} &= W^{[1]}\\cdot X + b^{[1]} \\\\\n",
    "A^{[1]} &= g_{ReLU}(Z^{[1]})\n",
    "\\end{align*}\n",
    "\n",
    "The equations for the second layer or the output layer,\n",
    "\n",
    "\\begin{align*}\n",
    "Z^{[2]} &= W^{[2]}\\cdot A^{[1]} + b^{[2]} \\\\\n",
    "A^{[2]} &= g_{softmax}(Z^{[2]})\n",
    "\\end{align*}\n",
    "\n",
    "where,\\\n",
    "\\\n",
    "$X$ or $A^{[0]}$ will be our *x_train* array with dimensions $784\\times60000$ \\\n",
    "$W^{[1]}$ is the weight matrix of the first hidden layer and has dimensions $16\\times784$, as there are 16 nodes and the dot product $(W^{[1]}\\cdot X)$ results with a matrix of dimensions $16\\times60000$ \\\n",
    "$b^{[1]}$ is is the bias matrix for the first hidden layer and has dimensions $16\\times1$, broadcasting during matrix addition results in $Z^{[1]}$ with dimensions of $16\\times60000$ \\\n",
    "\\\n",
    "Passing $Z^{[1]}$ through a softmax activation function results in $A^{[1]}$ with dimensions $16\\times60000$, this will be the input to the output layer $Z^{[2]}$ \\\n",
    "The weight matrix for the output layer with 10 nodes for the 10 classes $W^{[2]}$ will have dimensions $10\\times16$, as it is recieves 16 features from the first hidden layer. The resulting dot product of $(W^{[2]}\\cdot A^{[1]})$ will end up having dimensions $10\\times60000$ and the bias matrix for the output layer $b^{[2]}$ will have dimensions $10\\times1$. \\\n",
    "\\\n",
    "The resulting $Z^{[2]}$ will have dimensions $10\\times60000$ which is then passed through a softmax function to get the probabilities of each class (our output). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weight matrices are initialized randomly and the bias matrices are initialized as zero for the first pass \n",
    "def initialize_params(input_layer_nodes=784,hidden_layer_nodes=16,output_layer_nodes=10):\n",
    "    W1 = np.random.rand(hidden_layer_nodes, input_layer_nodes) * 0.1\n",
    "    b1 = np.zeros((hidden_layer_nodes, 1))\n",
    "    W2 = np.random.rand(output_layer_nodes, hidden_layer_nodes) * 0.1\n",
    "    b2 = np.zeros((output_layer_nodes, 1))\n",
    "    return W1, b1, W2, b2\n",
    "# Rectified linear activation function\n",
    "def ReLU(Z):\n",
    "    A = np.maximum(Z, 0)\n",
    "    return A\n",
    "\n",
    "# Softmax activation function\n",
    "def Softmax(Z):\n",
    "    A = np.exp(Z)/sum(np.exp(Z))\n",
    "    return A\n",
    "# Forward propogation\n",
    "def forward(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 =W2.dot(A1) + b2\n",
    "    A2 = Softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find parameters of $W$ and $b$ such that we minimize the cross-entropy loss function $L$ given by, \n",
    "\n",
    "\n",
    "$$L = - \\sum_{i}Y_{i}log(A_{i}^{[2]})$$\n",
    "\n",
    "where $Y_{i}$ is one-hot encoded. We do this using gradient descent - the gradient of the loss in the output layer:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial Z^{[2]}} = \\frac{\\partial L}{\\partial A^{[2]}} \\cdot \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} $$\n",
    "\n",
    "this simplifies to\n",
    "\n",
    "$$dZ^{[2]} = \\frac{\\partial L}{\\partial Z^{[2]}} = A^{[2]} - Y$$\n",
    "\n",
    "The gradient of the loss in the output layer weights is,\n",
    "\n",
    "$$dW^{[2]} = \\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}}$$\n",
    "\n",
    "which becomes:\n",
    "\n",
    "$$dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T} $$\n",
    "\n",
    "as $$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$\n",
    "\n",
    "$$db^{[2]} = \\frac{1}{m}\\sum dZ^{[2]}$$\n",
    "\n",
    "Similarly for the hidden layer,\n",
    "$$dZ^{[1]} = W^{[2]T}dZ^{[2]} \\cdot g^{[1]'}Z^{[1]}$$\n",
    "\n",
    "$$dW^{[1]} = \\frac{1}{m}dZ^{[1]}A^{[0]T}$$\n",
    "\n",
    "$$db^{[1]} = \\frac{1}{m}\\sum dZ{[1]}$$\n",
    "\n",
    "where $g^{[1]'}$ is the derivative of the ReLU function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for one hot encoding\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "# Function for the derivative of the ReLU function\n",
    "def derivative_ReLU(Z):\n",
    "    return Z>0\n",
    "\n",
    "# Backward propogation\n",
    "def backward(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    dZ2 = A2 - one_hot(Y)\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * derivative_ReLU(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W^{[2]}_{new} = W{[2]} - \\alpha dW^{[2]}$$\n",
    "\n",
    "$$b^{[2]}_{new} = b^{[2]} - \\alpha db^{[2]}$$\n",
    "\n",
    "$$W^{[1]}_{new} = W{[1]} - \\alpha dW^{[1]}$$\n",
    "\n",
    "$$b^{[1]}_{new} = b^{[1]} - \\alpha db^{[1]}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print logs\n",
    "def log_box(iteration, accuracy):\n",
    "    iteration_str = f\"Iteration: {iteration}\"\n",
    "    accuracy_str = f\"Accuracy: {accuracy:.2f}\"\n",
    "    max_length = max(len(iteration_str), len(accuracy_str))\n",
    "\n",
    "    pad_left = (max_length - len(iteration_str)) // 2\n",
    "    pad_right = max_length - len(iteration_str) - pad_left\n",
    "\n",
    "    border = '#' * (max_length + 4)\n",
    "    print(border)\n",
    "    print('#' + ' ' * (pad_left + 1) + iteration_str + ' ' * (pad_right + 1) + '#')\n",
    "    print('# ' + accuracy_str + ' ' * (max_length - len(accuracy_str)) + ' #')\n",
    "    print(border)\n",
    "\n",
    "\n",
    "\n",
    "# Functions to calculate accuracy\n",
    "def get_accuracy(Y_hat, Y):\n",
    "    return np.sum(Y_hat == Y) / Y.size\n",
    "\n",
    "def learn(X, Y, learning_rate, steps):\n",
    "    W1, b1, W2, b2 = initialize_params()\n",
    "    for i in range(steps):\n",
    "        Z1, A1, Z2, A2 = forward(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        # Log accuracy every 10 steps\n",
    "        if i % 50 == 0:\n",
    "            Y_hat = np.argmax(A2, 0)\n",
    "            accuracy = get_accuracy(Y_hat, Y)\n",
    "            log_box(i, accuracy)\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "#  Iteration: 0  #\n",
      "# Accuracy: 0.10 #\n",
      "##################\n",
      "##################\n",
      "# Iteration: 50  #\n",
      "# Accuracy: 0.57 #\n",
      "##################\n",
      "##################\n",
      "# Iteration: 100 #\n",
      "# Accuracy: 0.80 #\n",
      "##################\n",
      "##################\n",
      "# Iteration: 150 #\n",
      "# Accuracy: 0.84 #\n",
      "##################\n",
      "##################\n",
      "# Iteration: 200 #\n",
      "# Accuracy: 0.86 #\n",
      "##################\n",
      "##################\n",
      "# Iteration: 250 #\n",
      "# Accuracy: 0.87 #\n",
      "##################\n",
      "##################\n",
      "# Iteration: 300 #\n",
      "# Accuracy: 0.88 #\n",
      "##################\n",
      "##################\n",
      "# Iteration: 350 #\n",
      "# Accuracy: 0.89 #\n",
      "##################\n",
      "##################\n",
      "# Iteration: 400 #\n",
      "# Accuracy: 0.89 #\n",
      "##################\n",
      "##################\n",
      "# Iteration: 450 #\n",
      "# Accuracy: 0.90 #\n",
      "##################\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learning_rate = 0.1\n",
    "steps = 500\n",
    "W1, b1, W2, b2 = learn(x_train/255, y_train, learning_rate, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
