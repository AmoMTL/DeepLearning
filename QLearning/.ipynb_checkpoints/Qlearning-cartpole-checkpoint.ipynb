{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc0f357-1f7f-4c14-a14d-89ae6b3e580d",
   "metadata": {},
   "source": [
    "# Q learning to solve cartpole environment on OpenAI's gym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1dce958-2649-41b1-8cf6-02e1c1087fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b0dba-cca4-451a-8922-77bf93a7aad7",
   "metadata": {},
   "source": [
    "## Bellman's Equation\n",
    "\n",
    "![Bellman's Equation](https://miro.medium.com/v2/resize:fit:750/format:webp/0*GBCjjZRYwpSvTu8U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565dda5d-26c4-4a31-9632-a7ceff13e14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Create the cart pole environment\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "98318cf7-1cbd-48ff-b8b8-b2efcf5e50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the required variables\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "discount = 0.95\n",
    "episodes = 60000\n",
    "total_time = 0\n",
    "total_reward = 0\n",
    "prior_episode_reward = 0\n",
    "\n",
    "observation = [150,150,200,150]\n",
    "np_array_win_size = np.array([0.25,0.25,0.01,0.1])\n",
    "\n",
    "epsilon = 1\n",
    "\n",
    "epsilon_decay = 0.99995\n",
    "decay_from_episode = 10000\n",
    "\n",
    "render_step = 1000 # render every n episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c7b40b85-9e2e-4970-a478-84f5a4550e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 200, 150, 2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the Q table\n",
    "\n",
    "q_table = np.random.uniform(low=0, high=1, size=(observation + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a302e316-842e-4c5e-9b45-936feb31d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get discreet state\n",
    "\n",
    "def get_discreet_state(state):\n",
    "    discreet_state = state[0]/np_array_win_size + np.array([15,10,10,19])\n",
    "    return tuple(discreet_state.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c7200493-f1fc-49c8-8068-7be4d4124093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000\n",
      "Average episode reward: 22.192\n",
      "Average episode time: 0.03664329147338867\n",
      "Episode: 2000\n",
      "Average episode reward: 44.563\n",
      "Average episode time: 0.0732880642414093\n",
      "Episode: 3000\n",
      "Average episode reward: 66.433\n",
      "Average episode time: 0.10910961675643921\n",
      "Episode: 4000\n",
      "Average episode reward: 88.683\n",
      "Average episode time: 0.14562257647514343\n",
      "Episode: 5000\n",
      "Average episode reward: 111.348\n",
      "Average episode time: 0.1827344264984131\n",
      "Episode: 6000\n",
      "Average episode reward: 134.008\n",
      "Average episode time: 0.21995615482330322\n",
      "Episode: 7000\n",
      "Average episode reward: 156.452\n",
      "Average episode time: 0.25628769969940185\n",
      "Episode: 8000\n",
      "Average episode reward: 178.674\n",
      "Average episode time: 0.29239729809761045\n",
      "Episode: 9000\n",
      "Average episode reward: 200.892\n",
      "Average episode time: 0.32881074142456057\n",
      "Episode: 10000\n",
      "Average episode reward: 223.094\n",
      "Average episode time: 0.364853901386261\n",
      "Episode: 11000\n",
      "Average episode reward: 245.643\n",
      "Average episode time: 0.401360946893692\n",
      "Episode: 12000\n",
      "Average episode reward: 268.531\n",
      "Average episode time: 0.4384618012905121\n",
      "Episode: 13000\n",
      "Average episode reward: 290.756\n",
      "Average episode time: 0.47455660891532897\n",
      "Episode: 14000\n",
      "Average episode reward: 312.122\n",
      "Average episode time: 0.509265009880066\n",
      "Episode: 15000\n",
      "Average episode reward: 334.01\n",
      "Average episode time: 0.5444401404857635\n",
      "Episode: 16000\n",
      "Average episode reward: 355.346\n",
      "Average episode time: 0.5794299278259277\n",
      "Episode: 17000\n",
      "Average episode reward: 376.298\n",
      "Average episode time: 0.6138662581443787\n",
      "Episode: 18000\n",
      "Average episode reward: 397.301\n",
      "Average episode time: 0.6480271785259247\n",
      "Episode: 19000\n",
      "Average episode reward: 417.989\n",
      "Average episode time: 0.6823027384281158\n",
      "Episode: 20000\n",
      "Average episode reward: 438.295\n",
      "Average episode time: 0.7159277956485748\n",
      "Episode: 21000\n",
      "Average episode reward: 458.934\n",
      "Average episode time: 0.7506856832504273\n",
      "Episode: 22000\n",
      "Average episode reward: 479.095\n",
      "Average episode time: 0.7843136985301972\n",
      "Episode: 23000\n",
      "Average episode reward: 498.888\n",
      "Average episode time: 0.8166192178726196\n",
      "Episode: 24000\n",
      "Average episode reward: 518.414\n",
      "Average episode time: 0.8495737309455872\n",
      "Episode: 25000\n",
      "Average episode reward: 539.02\n",
      "Average episode time: 0.8839142765998841\n",
      "Episode: 26000\n",
      "Average episode reward: 558.919\n",
      "Average episode time: 0.9174596626758575\n",
      "Episode: 27000\n",
      "Average episode reward: 578.082\n",
      "Average episode time: 0.9504743280410767\n",
      "Episode: 28000\n",
      "Average episode reward: 597.287\n",
      "Average episode time: 0.9829092450141906\n",
      "Episode: 29000\n",
      "Average episode reward: 617.239\n",
      "Average episode time: 1.0159392898082733\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1,episodes+1):\n",
    "    t_start = time.time() # start time\n",
    "    discreet_state = get_discreet_state(env.reset()) # starting discreet state\n",
    "    done = False # reset flag\n",
    "    episode_reward = 0 # reset episode rewards to zero at start of each new episode\n",
    "\n",
    "    if episode % render_step == 0: # render every 200th episode\n",
    "        print(f\"Episode: {episode}\")\n",
    "        mean_episode_time = total_time / render_step\n",
    "        mean_episode_reward = total_reward / render_step\n",
    "        print(f\"Average episode reward: {mean_episode_reward}\")\n",
    "        print(f\"Average episode time: {mean_episode_time}\")\n",
    "        if mean_episode_reward > 200 and epsilon < 0.4:\n",
    "            break\n",
    "\n",
    "    while not done: # play the episode\n",
    "        ##### Taking Action ######\n",
    "        if np.random.random() > epsilon:  # if late in to the number of episodes\n",
    "            action = np.argmax(q_table[discreet_state]) # take action from mature Q table\n",
    "\n",
    "        else:  # build the Q table in the early episodes by taking random action\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "        # take step / next frame in episode\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        episode_reward += 1\n",
    "\n",
    "        new_discreet_state = get_discreet_state(new_state)\n",
    "\n",
    "        if episode % render_step == 0:\n",
    "            env.render()\n",
    "\n",
    "        # update Q table\n",
    "        if not done:\n",
    "\n",
    "            max_future_q = np.max(q_table[discreet_state])\n",
    "\n",
    "            current_q = q_table[discreet_state, action]\n",
    "\n",
    "            # Bellman's Equation to get new Q value\n",
    "            new_q = (1 - learning_rate) * current_q + learning_rate * (reward + discount * max_future_q)\n",
    "\n",
    "            q_table[discreet_state, (action, )] = new_q\n",
    "\n",
    "\n",
    "        discreet_state = new_discreet_state\n",
    "\n",
    "    ####### End of Episode ###########\n",
    "    t_end = time.time()\n",
    "    episode_time = t_end - t_start\n",
    "    total_time = total_time + episode_time\n",
    "\n",
    "    total_reward += episode_reward\n",
    "\n",
    "\n",
    "    \n",
    "    # Decay the epsilon\n",
    "    if epsilon > 0.05:\n",
    "        if episode_reward > prior_episode_reward and episode > decay_from_episode:\n",
    "            epsilon = math.pow(epsilon_decay, episode - decay_from_episode)\n",
    "\n",
    "    prior_reward = episode_reward\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a7f16e9f-e475-46e7-bd8b-61c80a5db004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4809636 ,  2.180926  ,  0.10811789, -0.22904956], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e6a50616-79b2-4f33-9530-d9eda3f47194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38675117571691847"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "684035b3-d8b2-4808-b01e-a47af80032b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 15, 153, 33)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discreet_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3584e3-d7a8-4a9c-8014-27838ecfaeff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
